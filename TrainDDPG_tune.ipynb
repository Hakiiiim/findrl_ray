{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4ba7f2b6-480a-4462-af82-80e6d588cd8a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle \n",
    "import os\n",
    "import finrl\n",
    "from finenv.env_stocktrading import StockTradingEnv\n",
    "from finenv.preprocessors import data_split\n",
    "from finenv.save_model import *\n",
    "import psutil\n",
    "import ray\n",
    "from datetime import datetime\n",
    "ray._private.utils.get_system_memory = lambda: psutil.virtual_memory().total\n",
    "from ray.tune.registry import register_env\n",
    "from gymnasium.wrappers import EnvCompatibility\n",
    "from ray.rllib.agents import ppo\n",
    "from ray.rllib.algorithms.td3 import TD3Config\n",
    "\n",
    "#os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "#os.environ[\"CUBLAS_WORKSPACE_CONFIG\"]=\":4096:8\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b7882d0a-464f-46ed-8be6-e22b4ba58596",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stock Dimension: 1, State Space: 7\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>tic</th>\n",
       "      <th>macd</th>\n",
       "      <th>rsi</th>\n",
       "      <th>cci</th>\n",
       "      <th>adx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-01-04 09:30</td>\n",
       "      <td>110.26</td>\n",
       "      <td>110.26</td>\n",
       "      <td>110.22</td>\n",
       "      <td>110.24</td>\n",
       "      <td>12700.0</td>\n",
       "      <td>QQQ</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016-01-04 09:45</td>\n",
       "      <td>110.14</td>\n",
       "      <td>110.14</td>\n",
       "      <td>110.00</td>\n",
       "      <td>110.00</td>\n",
       "      <td>200.0</td>\n",
       "      <td>QQQ</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016-01-04 10:00</td>\n",
       "      <td>110.07</td>\n",
       "      <td>110.07</td>\n",
       "      <td>110.07</td>\n",
       "      <td>110.07</td>\n",
       "      <td>400.0</td>\n",
       "      <td>QQQ</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016-01-04 10:15</td>\n",
       "      <td>110.05</td>\n",
       "      <td>110.05</td>\n",
       "      <td>110.02</td>\n",
       "      <td>110.03</td>\n",
       "      <td>700.0</td>\n",
       "      <td>QQQ</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016-01-04 10:30</td>\n",
       "      <td>109.95</td>\n",
       "      <td>110.03</td>\n",
       "      <td>109.95</td>\n",
       "      <td>110.03</td>\n",
       "      <td>800.0</td>\n",
       "      <td>QQQ</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               date    open    high     low   close   volume  tic  macd  rsi  \\\n",
       "0  2016-01-04 09:30  110.26  110.26  110.22  110.24  12700.0  QQQ   0.0  0.0   \n",
       "1  2016-01-04 09:45  110.14  110.14  110.00  110.00    200.0  QQQ   0.0  0.0   \n",
       "2  2016-01-04 10:00  110.07  110.07  110.07  110.07    400.0  QQQ   0.0  0.0   \n",
       "3  2016-01-04 10:15  110.05  110.05  110.02  110.03    700.0  QQQ   0.0  0.0   \n",
       "4  2016-01-04 10:30  109.95  110.03  109.95  110.03    800.0  QQQ   0.0  0.0   \n",
       "\n",
       "   cci  adx  \n",
       "0  0.0  0.0  \n",
       "1  0.0  0.0  \n",
       "2  0.0  0.0  \n",
       "3  0.0  0.0  \n",
       "4  0.0  0.0  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv = False\n",
    "if csv:\n",
    "    train = pd.read_csv('dataset/train_data.csv')\n",
    "    train = train.set_index(train.columns[0])\n",
    "    train.index.names = ['']\n",
    "    INDICATORS = ['macd','boll_ub','boll_lb','rsi_30','cci_30','dx_30','close_30_sma','close_60_sma']\n",
    "\n",
    "else: \n",
    "    train = pd.read_pickle('dataset/qqq_train.pkl')\n",
    "    train['date'] = pd.to_datetime(train['date'])\n",
    "    train['date'] = train['date'].dt.strftime('%Y-%m-%d %H:%M')\n",
    "    INDICATORS = ['macd','rsi','cci','adx']\n",
    "    \n",
    "   \n",
    "stock_dimension = len(train.tic.unique())\n",
    "state_space = 1 + 2*stock_dimension + len(INDICATORS)*stock_dimension\n",
    "print(f\"Stock Dimension: {stock_dimension}, State Space: {state_space}\")\n",
    "train.head(5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb6781a9-96f3-4a91-8819-960f699e2438",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def env_creator(env_config):\n",
    "    # env_config is passed as {} and defaults are set here\n",
    "    df = env_config.get('df', train)\n",
    "    hmax = env_config.get('hmax', 200)\n",
    "    initial_amount = env_config.get('initial_amount', 1000000)\n",
    "    num_stock_shares = env_config.get('num_stock_shares', [0] * stock_dimension)\n",
    "    buy_cost_pct = env_config.get('buy_cost_pct', buy_cost_list)\n",
    "    sell_cost_pct = env_config.get('sell_cost_pct', sell_cost_list)\n",
    "    state_space = env_config.get('state_space', 1 + 2*stock_dimension + len(INDICATORS)*stock_dimension)\n",
    "    stock_dim = env_config.get('stock_dim', stock_dimension)\n",
    "    tech_indicator_list = env_config.get('tech_indicator_list', INDICATORS)\n",
    "    action_space = env_config.get('action_space', stock_dimension)\n",
    "    reward_scaling = env_config.get('reward_scaling', 0.999)\n",
    "\n",
    "    return EnvCompatibility(StockTradingEnv(\n",
    "        df=df,\n",
    "        hmax=hmax,\n",
    "        initial_amount=initial_amount,\n",
    "        num_stock_shares=num_stock_shares,\n",
    "        buy_cost_pct=buy_cost_pct,\n",
    "        sell_cost_pct=sell_cost_pct,\n",
    "        state_space=state_space,\n",
    "        stock_dim=stock_dim,\n",
    "        tech_indicator_list=tech_indicator_list,\n",
    "        action_space=action_space,\n",
    "        reward_scaling=reward_scaling\n",
    "    ))\n",
    "\n",
    "register_env(\"finrl\", env_creator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a3d58e55-a77f-4d4a-843b-e4291e9fd2f9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ray is being initialized\n"
     ]
    }
   ],
   "source": [
    "ray.shutdown()\n",
    "#ray.init(num_cpus=122,dashboard_port=8080)\n",
    "print(f\"ray is being initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "116a4f64-e0e5-4204-bb9c-0f6c8d2fa2ed",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 1, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'num_trainer_workers': 0, 'num_gpus_per_trainer_worker': 0, 'num_cpus_per_trainer_worker': 1, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': None, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'is_atari': None, 'auto_wrap_old_gym_envs': True, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': True, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, 'gamma': 0.99, 'lr': 0.01, 'train_batch_size': 1024, 'model': {'_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': True, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, 'rl_trainer_class': None, '_enable_rl_trainer_api': False, '_rl_trainer_hps': RLTrainerHPs(), 'explore': True, 'exploration_config': {'type': 'GaussianNoise', 'random_timesteps': 10000, 'stddev': 0.1, 'initial_scale': 1.0, 'final_scale': 1.0, 'scale_timesteps': 1}, 'policies': {'default_policy': <ray.rllib.policy.policy.PolicySpec object at 0x7fac204edf70>}, 'policy_states_are_swappable': False, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {'explore': False}, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 1000, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, 'worker_cls': None, 'rl_module_class': None, '_enable_rl_module_api': False, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': -1, 'replay_sequence_length': None, 'horizon': -1, 'soft_horizon': -1, 'no_done_at_end': -1, 'target_network_update_freq': 0, 'replay_buffer_config': {'type': 'MultiAgentReplayBuffer', 'prioritized_replay': -1, 'capacity': 1000000, 'worker_side_prioritization': False}, 'num_steps_sampled_before_learning_starts': 10000, 'store_buffer_in_checkpoints': False, 'lr_schedule': None, 'adam_epsilon': 1e-08, 'grad_clip': None, 'tau': 0.005, 'twin_q': True, 'policy_delay': 2, 'smooth_target_policy': (True,), 'target_noise': 0.2, 'target_noise_clip': 0.5, 'use_state_preprocessor': False, 'actor_hiddens': [400, 300], 'actor_hidden_activation': 'relu', 'critic_hiddens': [400, 300], 'critic_hidden_activation': 'relu', 'n_step': 1, 'training_intensity': None, 'critic_lr': 0.001, 'actor_lr': 0.001, 'use_huber': False, 'huber_threshold': 1.0, 'l2_reg': 0.0, 'worker_side_prioritization': -1, 'input': 'sampler', 'multiagent': {'policies': {'default_policy': (None, None, None, None)}, 'policy_mapping_fn': <function AlgorithmConfig.__init__.<locals>.<lambda> at 0x7faacb8fc040>, 'policies_to_train': None, 'policy_map_capacity': 100, 'policy_map_cache': -1, 'count_steps_by': 'env_steps', 'observation_fn': None}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 10}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-07 22:24:01,127\tINFO worker.py:1544 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18924)\u001b[0m 2023-04-07 22:24:13,073\tWARNING env.py:156 -- Your env doesn't have a .spec.max_episode_steps attribute. Your horizon will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18924)\u001b[0m 2023-04-07 22:24:13,073\tWARNING env.py:166 -- Your env reset() method appears to take 'seed' or 'return_info' arguments. Note that these are not yet supported in RLlib. Seeding will take place using 'env.seed()' and the info dict will not be returned from reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18924)\u001b[0m /home/ga_aiot/anaconda3/envs/finrl/lib/python3.8/site-packages/gymnasium/spaces/box.py:227: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18924)\u001b[0m   logger.warn(\"Casting input x to numpy array.\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18951)\u001b[0m /home/ga_aiot/anaconda3/envs/finrl/lib/python3.8/site-packages/gymnasium/spaces/box.py:227: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18951)\u001b[0m   logger.warn(\"Casting input x to numpy array.\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18979)\u001b[0m /home/ga_aiot/anaconda3/envs/finrl/lib/python3.8/site-packages/gymnasium/spaces/box.py:227: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18979)\u001b[0m   logger.warn(\"Casting input x to numpy array.\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18964)\u001b[0m /home/ga_aiot/anaconda3/envs/finrl/lib/python3.8/site-packages/gymnasium/spaces/box.py:227: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18964)\u001b[0m   logger.warn(\"Casting input x to numpy array.\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19042)\u001b[0m /home/ga_aiot/anaconda3/envs/finrl/lib/python3.8/site-packages/gymnasium/spaces/box.py:227: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19042)\u001b[0m   logger.warn(\"Casting input x to numpy array.\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18937)\u001b[0m /home/ga_aiot/anaconda3/envs/finrl/lib/python3.8/site-packages/gymnasium/spaces/box.py:227: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18937)\u001b[0m   logger.warn(\"Casting input x to numpy array.\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19028)\u001b[0m /home/ga_aiot/anaconda3/envs/finrl/lib/python3.8/site-packages/gymnasium/spaces/box.py:227: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19028)\u001b[0m   logger.warn(\"Casting input x to numpy array.\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19159)\u001b[0m /home/ga_aiot/anaconda3/envs/finrl/lib/python3.8/site-packages/gymnasium/spaces/box.py:227: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19159)\u001b[0m   logger.warn(\"Casting input x to numpy array.\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19080)\u001b[0m /home/ga_aiot/anaconda3/envs/finrl/lib/python3.8/site-packages/gymnasium/spaces/box.py:227: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19080)\u001b[0m   logger.warn(\"Casting input x to numpy array.\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19130)\u001b[0m /home/ga_aiot/anaconda3/envs/finrl/lib/python3.8/site-packages/gymnasium/spaces/box.py:227: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19130)\u001b[0m   logger.warn(\"Casting input x to numpy array.\")\n",
      "/home/ga_aiot/anaconda3/envs/finrl/lib/python3.8/site-packages/ray/rllib/algorithms/ddpg/ddpg_torch_model.py:110: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:199.)\n",
      "  torch.from_numpy(self.action_space.low).float()\n",
      "2023-04-07 22:24:17,465\tINFO trainable.py:172 -- Trainable.setup took 18.599 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n"
     ]
    }
   ],
   "source": [
    "from ray.rllib.algorithms.ddpg.ddpg import DDPGConfig\n",
    "#config = DDPGConfig().training(lr=0.01).resources(num_gpus=1).framework(framework=\"torch\").rollouts(num_rollout_workers=10)\n",
    "config = TD3Config().training(lr=0.01).resources(num_gpus=1).framework(framework=\"torch\").rollouts(num_rollout_workers=10)\n",
    "config[\"model\"][\"fcnet_hiddens\"] = [256, 256, 256]\n",
    "config['train_batch_size'] = 1024\n",
    "print(config.to_dict())  \n",
    "# Build a Trainer object from the config and run one training iteration.\n",
    "trainer = config.build(env=\"finrl\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "67da4c9a-dcef-4c02-a17a-31a5e1b4c0cb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "640c919c4775483e86db31b909f7c3ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Episodes:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Rwd:nan\n",
      "Checkpoint5 saved in directory model/ddpg_230407\n",
      "Mean Rwd:nan\n",
      "Checkpoint10 saved in directory model/ddpg_230407\n",
      "Mean Rwd:4345951.072325179\n",
      "Checkpoint15 saved in directory model/ddpg_230407\n",
      "Mean Rwd:4345951.072325179\n",
      "Checkpoint20 saved in directory model/ddpg_230407\n",
      "Mean Rwd:4345951.072325179\n",
      "Checkpoint25 saved in directory model/ddpg_230407\n",
      "Mean Rwd:4345951.072325179\n",
      "Checkpoint30 saved in directory model/ddpg_230407\n",
      "Mean Rwd:4345951.072325179\n",
      "Checkpoint35 saved in directory model/ddpg_230407\n",
      "Mean Rwd:2614410.1976177725\n",
      "Checkpoint40 saved in directory model/ddpg_230407\n",
      "Mean Rwd:2614410.1976177725\n",
      "Checkpoint45 saved in directory model/ddpg_230407\n",
      "Mean Rwd:2614410.1976177725\n",
      "Checkpoint50 saved in directory model/ddpg_230407\n",
      "Mean Rwd:2614410.1976177725\n",
      "Checkpoint55 saved in directory model/ddpg_230407\n",
      "Mean Rwd:2043918.6133481797\n",
      "Checkpoint60 saved in directory model/ddpg_230407\n",
      "Mean Rwd:2043918.6133481797\n",
      "Checkpoint65 saved in directory model/ddpg_230407\n",
      "Mean Rwd:2043918.6133481797\n",
      "Checkpoint70 saved in directory model/ddpg_230407\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-07 22:58:30,348\tERROR actor_manager.py:496 -- Ray error, taking actor 10 out of service. The actor died unexpectedly before finishing this task.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20877)\u001b[0m /home/ga_aiot/anaconda3/envs/finrl/lib/python3.8/site-packages/gymnasium/spaces/box.py:227: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20877)\u001b[0m   logger.warn(\"Casting input x to numpy array.\")\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-07 22:59:01,147 E 17548 17548] (raylet) node_manager.cc:3040: 1 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "2023-04-07 23:00:16,483\tERROR actor_manager.py:496 -- Ray error, taking actor 10 out of service. The actor died unexpectedly before finishing this task.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=6337)\u001b[0m /home/ga_aiot/anaconda3/envs/finrl/lib/python3.8/site-packages/gymnasium/spaces/box.py:227: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=6337)\u001b[0m   logger.warn(\"Casting input x to numpy array.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Rwd:2043918.6133481797\n",
      "Checkpoint75 saved in directory model/ddpg_230407\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-07 23:01:01,149 E 17548 17548] (raylet) node_manager.cc:3040: 1 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "2023-04-07 23:01:09,521\tERROR actor_manager.py:496 -- Ray error, taking actor 10 out of service. The actor died unexpectedly before finishing this task.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15608)\u001b[0m /home/ga_aiot/anaconda3/envs/finrl/lib/python3.8/site-packages/gymnasium/spaces/box.py:227: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15608)\u001b[0m   logger.warn(\"Casting input x to numpy array.\")\n",
      "2023-04-07 23:01:47,572\tERROR actor_manager.py:496 -- Ray error, taking actor 10 out of service. The actor died unexpectedly before finishing this task.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=12227)\u001b[0m /home/ga_aiot/anaconda3/envs/finrl/lib/python3.8/site-packages/gymnasium/spaces/box.py:227: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=12227)\u001b[0m   logger.warn(\"Casting input x to numpy array.\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19982)\u001b[0m /home/ga_aiot/anaconda3/envs/finrl/lib/python3.8/site-packages/gymnasium/spaces/box.py:227: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19982)\u001b[0m   logger.warn(\"Casting input x to numpy array.\")\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-07 23:02:01,174 E 17548 17548] (raylet) node_manager.cc:3040: 4 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=22761)\u001b[0m /home/ga_aiot/anaconda3/envs/finrl/lib/python3.8/site-packages/gymnasium/spaces/box.py:227: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=22761)\u001b[0m   logger.warn(\"Casting input x to numpy array.\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=27186)\u001b[0m /home/ga_aiot/anaconda3/envs/finrl/lib/python3.8/site-packages/gymnasium/spaces/box.py:227: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=27186)\u001b[0m   logger.warn(\"Casting input x to numpy array.\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=31694)\u001b[0m /home/ga_aiot/anaconda3/envs/finrl/lib/python3.8/site-packages/gymnasium/spaces/box.py:227: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=31694)\u001b[0m   logger.warn(\"Casting input x to numpy array.\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=6605)\u001b[0m /home/ga_aiot/anaconda3/envs/finrl/lib/python3.8/site-packages/gymnasium/spaces/box.py:227: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=6605)\u001b[0m   logger.warn(\"Casting input x to numpy array.\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11152)\u001b[0m /home/ga_aiot/anaconda3/envs/finrl/lib/python3.8/site-packages/gymnasium/spaces/box.py:227: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11152)\u001b[0m   logger.warn(\"Casting input x to numpy array.\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=13948)\u001b[0m /home/ga_aiot/anaconda3/envs/finrl/lib/python3.8/site-packages/gymnasium/spaces/box.py:227: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=13948)\u001b[0m   logger.warn(\"Casting input x to numpy array.\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20321)\u001b[0m /home/ga_aiot/anaconda3/envs/finrl/lib/python3.8/site-packages/gymnasium/spaces/box.py:227: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20321)\u001b[0m   logger.warn(\"Casting input x to numpy array.\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=24597)\u001b[0m /home/ga_aiot/anaconda3/envs/finrl/lib/python3.8/site-packages/gymnasium/spaces/box.py:227: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=24597)\u001b[0m   logger.warn(\"Casting input x to numpy array.\")\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-07 23:03:01,175 E 17548 17548] (raylet) node_manager.cc:3040: 8 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=29090)\u001b[0m /home/ga_aiot/anaconda3/envs/finrl/lib/python3.8/site-packages/gymnasium/spaces/box.py:227: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=29090)\u001b[0m   logger.warn(\"Casting input x to numpy array.\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1215)\u001b[0m /home/ga_aiot/anaconda3/envs/finrl/lib/python3.8/site-packages/gymnasium/spaces/box.py:227: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1215)\u001b[0m   logger.warn(\"Casting input x to numpy array.\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5634)\u001b[0m /home/ga_aiot/anaconda3/envs/finrl/lib/python3.8/site-packages/gymnasium/spaces/box.py:227: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5634)\u001b[0m   logger.warn(\"Casting input x to numpy array.\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=8554)\u001b[0m /home/ga_aiot/anaconda3/envs/finrl/lib/python3.8/site-packages/gymnasium/spaces/box.py:227: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=8554)\u001b[0m   logger.warn(\"Casting input x to numpy array.\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=12811)\u001b[0m /home/ga_aiot/anaconda3/envs/finrl/lib/python3.8/site-packages/gymnasium/spaces/box.py:227: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=12811)\u001b[0m   logger.warn(\"Casting input x to numpy array.\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19899)\u001b[0m /home/ga_aiot/anaconda3/envs/finrl/lib/python3.8/site-packages/gymnasium/spaces/box.py:227: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19899)\u001b[0m   logger.warn(\"Casting input x to numpy array.\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=28940)\u001b[0m /home/ga_aiot/anaconda3/envs/finrl/lib/python3.8/site-packages/gymnasium/spaces/box.py:227: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=28940)\u001b[0m   logger.warn(\"Casting input x to numpy array.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Rwd:2043918.6133481797\n",
      "Checkpoint80 saved in directory model/ddpg_230407\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-07 23:03:52,731\tERROR actor_manager.py:496 -- Ray error, taking actor 10 out of service. The actor died unexpectedly before finishing this task.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1007)\u001b[0m /home/ga_aiot/anaconda3/envs/finrl/lib/python3.8/site-packages/gymnasium/spaces/box.py:227: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1007)\u001b[0m   logger.warn(\"Casting input x to numpy array.\")\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-07 23:04:01,176 E 17548 17548] (raylet) node_manager.cc:3040: 8 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3558)\u001b[0m /home/ga_aiot/anaconda3/envs/finrl/lib/python3.8/site-packages/gymnasium/spaces/box.py:227: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3558)\u001b[0m   logger.warn(\"Casting input x to numpy array.\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=7249)\u001b[0m /home/ga_aiot/anaconda3/envs/finrl/lib/python3.8/site-packages/gymnasium/spaces/box.py:227: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=7249)\u001b[0m   logger.warn(\"Casting input x to numpy array.\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11502)\u001b[0m /home/ga_aiot/anaconda3/envs/finrl/lib/python3.8/site-packages/gymnasium/spaces/box.py:227: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11502)\u001b[0m   logger.warn(\"Casting input x to numpy array.\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=17110)\u001b[0m /home/ga_aiot/anaconda3/envs/finrl/lib/python3.8/site-packages/gymnasium/spaces/box.py:227: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=17110)\u001b[0m   logger.warn(\"Casting input x to numpy array.\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=22671)\u001b[0m /home/ga_aiot/anaconda3/envs/finrl/lib/python3.8/site-packages/gymnasium/spaces/box.py:227: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=22671)\u001b[0m   logger.warn(\"Casting input x to numpy array.\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=26921)\u001b[0m /home/ga_aiot/anaconda3/envs/finrl/lib/python3.8/site-packages/gymnasium/spaces/box.py:227: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=26921)\u001b[0m   logger.warn(\"Casting input x to numpy array.\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=30341)\u001b[0m /home/ga_aiot/anaconda3/envs/finrl/lib/python3.8/site-packages/gymnasium/spaces/box.py:227: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=30341)\u001b[0m   logger.warn(\"Casting input x to numpy array.\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1583)\u001b[0m /home/ga_aiot/anaconda3/envs/finrl/lib/python3.8/site-packages/gymnasium/spaces/box.py:227: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1583)\u001b[0m   logger.warn(\"Casting input x to numpy array.\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=8926)\u001b[0m /home/ga_aiot/anaconda3/envs/finrl/lib/python3.8/site-packages/gymnasium/spaces/box.py:227: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=8926)\u001b[0m   logger.warn(\"Casting input x to numpy array.\")\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-07 23:05:01,177 E 17548 17548] (raylet) node_manager.cc:3040: 9 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=13286)\u001b[0m /home/ga_aiot/anaconda3/envs/finrl/lib/python3.8/site-packages/gymnasium/spaces/box.py:227: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=13286)\u001b[0m   logger.warn(\"Casting input x to numpy array.\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20008)\u001b[0m /home/ga_aiot/anaconda3/envs/finrl/lib/python3.8/site-packages/gymnasium/spaces/box.py:227: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20008)\u001b[0m   logger.warn(\"Casting input x to numpy array.\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=23633)\u001b[0m /home/ga_aiot/anaconda3/envs/finrl/lib/python3.8/site-packages/gymnasium/spaces/box.py:227: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=23633)\u001b[0m   logger.warn(\"Casting input x to numpy array.\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=28084)\u001b[0m /home/ga_aiot/anaconda3/envs/finrl/lib/python3.8/site-packages/gymnasium/spaces/box.py:227: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=28084)\u001b[0m   logger.warn(\"Casting input x to numpy array.\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=31630)\u001b[0m /home/ga_aiot/anaconda3/envs/finrl/lib/python3.8/site-packages/gymnasium/spaces/box.py:227: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=31630)\u001b[0m   logger.warn(\"Casting input x to numpy array.\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3431)\u001b[0m /home/ga_aiot/anaconda3/envs/finrl/lib/python3.8/site-packages/gymnasium/spaces/box.py:227: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3431)\u001b[0m   logger.warn(\"Casting input x to numpy array.\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=7662)\u001b[0m /home/ga_aiot/anaconda3/envs/finrl/lib/python3.8/site-packages/gymnasium/spaces/box.py:227: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=7662)\u001b[0m   logger.warn(\"Casting input x to numpy array.\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11880)\u001b[0m /home/ga_aiot/anaconda3/envs/finrl/lib/python3.8/site-packages/gymnasium/spaces/box.py:227: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11880)\u001b[0m   logger.warn(\"Casting input x to numpy array.\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15571)\u001b[0m /home/ga_aiot/anaconda3/envs/finrl/lib/python3.8/site-packages/gymnasium/spaces/box.py:227: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15571)\u001b[0m   logger.warn(\"Casting input x to numpy array.\")\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-07 23:06:01,177 E 17548 17548] (raylet) node_manager.cc:3040: 9 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=25180)\u001b[0m /home/ga_aiot/anaconda3/envs/finrl/lib/python3.8/site-packages/gymnasium/spaces/box.py:227: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=25180)\u001b[0m   logger.warn(\"Casting input x to numpy array.\")\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-07 23:07:01,178 E 17548 17548] (raylet) node_manager.cc:3040: 10 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Rwd:1778260.4858915722\n",
      "Checkpoint85 saved in directory model/ddpg_230407\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-07 23:08:01,180 E 17548 17548] (raylet) node_manager.cc:3040: 12 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-07 23:09:01,464 E 17548 17548] (raylet) node_manager.cc:3040: 15 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-07 23:10:01,465 E 17548 17548] (raylet) node_manager.cc:3040: 15 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Rwd:1778260.4858915722\n",
      "Checkpoint90 saved in directory model/ddpg_230407\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-07 23:11:01,466 E 17548 17548] (raylet) node_manager.cc:3040: 15 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-07 23:12:01,467 E 17548 17548] (raylet) node_manager.cc:3040: 17 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-07 23:13:01,467 E 17548 17548] (raylet) node_manager.cc:3040: 25 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Rwd:1778260.4858915722\n",
      "Checkpoint95 saved in directory model/ddpg_230407\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-07 23:14:01,469 E 17548 17548] (raylet) node_manager.cc:3040: 30 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-07 23:15:01,471 E 17548 17548] (raylet) node_manager.cc:3040: 25 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-07 23:16:01,597 E 17548 17548] (raylet) node_manager.cc:3040: 33 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-07 23:17:01,619 E 17548 17548] (raylet) node_manager.cc:3040: 39 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Rwd:1778260.4858915722\n",
      "Checkpoint100 saved in directory model/ddpg_230407\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-07 23:18:01,926 E 17548 17548] (raylet) node_manager.cc:3040: 37 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-07 23:19:01,927 E 17548 17548] (raylet) node_manager.cc:3040: 32 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-07 23:20:01,928 E 17548 17548] (raylet) node_manager.cc:3040: 36 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-07 23:21:01,929 E 17548 17548] (raylet) node_manager.cc:3040: 43 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "2023-04-07 23:21:22,064\tERROR actor_manager.py:496 -- Ray error, taking actor 6 out of service. The actor died unexpectedly before finishing this task.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=9291)\u001b[0m /home/ga_aiot/anaconda3/envs/finrl/lib/python3.8/site-packages/gymnasium/spaces/box.py:227: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=9291)\u001b[0m   logger.warn(\"Casting input x to numpy array.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Rwd:1612787.8865993274\n",
      "Checkpoint105 saved in directory model/ddpg_230407\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-07 23:22:01,931 E 17548 17548] (raylet) node_manager.cc:3040: 25 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-07 23:23:01,932 E 17548 17548] (raylet) node_manager.cc:3040: 18 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-07 23:24:02,173 E 17548 17548] (raylet) node_manager.cc:3040: 21 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Rwd:1612787.8865993274\n",
      "Checkpoint110 saved in directory model/ddpg_230407\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-07 23:25:02,174 E 17548 17548] (raylet) node_manager.cc:3040: 27 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-07 23:26:02,175 E 17548 17548] (raylet) node_manager.cc:3040: 35 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "2023-04-07 23:26:42,832\tERROR actor_manager.py:496 -- Ray error, taking actor 10 out of service. The actor died unexpectedly before finishing this task.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=22456)\u001b[0m /home/ga_aiot/anaconda3/envs/finrl/lib/python3.8/site-packages/gymnasium/spaces/box.py:227: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=22456)\u001b[0m   logger.warn(\"Casting input x to numpy array.\")\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-07 23:27:02,477 E 17548 17548] (raylet) node_manager.cc:3040: 41 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=29463)\u001b[0m /home/ga_aiot/anaconda3/envs/finrl/lib/python3.8/site-packages/gymnasium/spaces/box.py:227: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=29463)\u001b[0m   logger.warn(\"Casting input x to numpy array.\")\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-07 23:28:02,478 E 17548 17548] (raylet) node_manager.cc:3040: 24 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=8006)\u001b[0m /home/ga_aiot/anaconda3/envs/finrl/lib/python3.8/site-packages/gymnasium/spaces/box.py:227: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=8006)\u001b[0m   logger.warn(\"Casting input x to numpy array.\")\n",
      "2023-04-07 23:28:24,073\tERROR actor_manager.py:496 -- Ray error, taking actor 10 out of service. The actor died unexpectedly before finishing this task.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15417)\u001b[0m /home/ga_aiot/anaconda3/envs/finrl/lib/python3.8/site-packages/gymnasium/spaces/box.py:227: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15417)\u001b[0m   logger.warn(\"Casting input x to numpy array.\")\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-07 23:29:02,480 E 17548 17548] (raylet) node_manager.cc:3040: 27 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Rwd:1612787.8865993274\n",
      "Checkpoint115 saved in directory model/ddpg_230407\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=30203)\u001b[0m /home/ga_aiot/anaconda3/envs/finrl/lib/python3.8/site-packages/gymnasium/spaces/box.py:227: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=30203)\u001b[0m   logger.warn(\"Casting input x to numpy array.\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1522)\u001b[0m /home/ga_aiot/anaconda3/envs/finrl/lib/python3.8/site-packages/gymnasium/spaces/box.py:227: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1522)\u001b[0m   logger.warn(\"Casting input x to numpy array.\")\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-07 23:30:02,480 E 17548 17548] (raylet) node_manager.cc:3040: 20 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5273)\u001b[0m /home/ga_aiot/anaconda3/envs/finrl/lib/python3.8/site-packages/gymnasium/spaces/box.py:227: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5273)\u001b[0m   logger.warn(\"Casting input x to numpy array.\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=8535)\u001b[0m /home/ga_aiot/anaconda3/envs/finrl/lib/python3.8/site-packages/gymnasium/spaces/box.py:227: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=8535)\u001b[0m   logger.warn(\"Casting input x to numpy array.\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15420)\u001b[0m /home/ga_aiot/anaconda3/envs/finrl/lib/python3.8/site-packages/gymnasium/spaces/box.py:227: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15420)\u001b[0m   logger.warn(\"Casting input x to numpy array.\")\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-07 23:31:02,481 E 17548 17548] (raylet) node_manager.cc:3040: 20 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16380)\u001b[0m /home/ga_aiot/anaconda3/envs/finrl/lib/python3.8/site-packages/gymnasium/spaces/box.py:227: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16380)\u001b[0m   logger.warn(\"Casting input x to numpy array.\")\n",
      "2023-04-07 23:32:00,610\tERROR actor_manager.py:496 -- Ray error, taking actor 6 out of service. The actor died unexpectedly before finishing this task.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-07 23:32:02,482 E 17548 17548] (raylet) node_manager.cc:3040: 5 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-07 23:33:02,846 E 17548 17548] (raylet) node_manager.cc:3040: 35 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Rwd:1612787.8865993274\n",
      "Checkpoint120 saved in directory model/ddpg_230407\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-07 23:34:02,847 E 17548 17548] (raylet) node_manager.cc:3040: 34 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-07 23:35:02,848 E 17548 17548] (raylet) node_manager.cc:3040: 37 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-07 23:36:02,849 E 17548 17548] (raylet) node_manager.cc:3040: 39 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-07 23:37:03,154 E 17548 17548] (raylet) node_manager.cc:3040: 30 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "2023-04-07 23:37:25,507\tWARNING worker.py:1866 -- A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: ffffffffffffffff39a79e424020edeae3d1a95601000000 Worker ID: 217290f8701881ba51b2668c91a01106d71ea7332b52a55525eef3d3 Node ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c Worker IP address: 172.21.0.25 Worker port: 35991 Worker PID: 32046 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\u001b[2m\u001b[36m(pid=32046)\u001b[0m Inconsistency detected by ld.so: ../elf/dl-tls.c: 517: _dl_allocate_tls_init: Assertion `listp != NULL' failed!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Rwd:1513110.0308006166\n",
      "Checkpoint125 saved in directory model/ddpg_230407\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=10073)\u001b[0m /home/ga_aiot/anaconda3/envs/finrl/lib/python3.8/site-packages/gymnasium/spaces/box.py:227: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=10073)\u001b[0m   logger.warn(\"Casting input x to numpy array.\")\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-07 23:38:03,155 E 17548 17548] (raylet) node_manager.cc:3040: 31 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=13154)\u001b[0m /home/ga_aiot/anaconda3/envs/finrl/lib/python3.8/site-packages/gymnasium/spaces/box.py:227: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=13154)\u001b[0m   logger.warn(\"Casting input x to numpy array.\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=31357)\u001b[0m /home/ga_aiot/anaconda3/envs/finrl/lib/python3.8/site-packages/gymnasium/spaces/box.py:227: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=31357)\u001b[0m   logger.warn(\"Casting input x to numpy array.\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=7155)\u001b[0m /home/ga_aiot/anaconda3/envs/finrl/lib/python3.8/site-packages/gymnasium/spaces/box.py:227: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=7155)\u001b[0m   logger.warn(\"Casting input x to numpy array.\")\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-07 23:39:03,156 E 17548 17548] (raylet) node_manager.cc:3040: 38 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-07 23:40:03,160 E 17548 17548] (raylet) node_manager.cc:3040: 41 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-07 23:41:03,161 E 17548 17548] (raylet) node_manager.cc:3040: 42 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-07 23:42:03,162 E 17548 17548] (raylet) node_manager.cc:3040: 50 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "2023-04-07 23:42:21,962\tERROR actor_manager.py:496 -- Ray error, taking actor 9 out of service. The actor died unexpectedly before finishing this task.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=32601)\u001b[0m /home/ga_aiot/anaconda3/envs/finrl/lib/python3.8/site-packages/gymnasium/spaces/box.py:227: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=32601)\u001b[0m   logger.warn(\"Casting input x to numpy array.\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=392)\u001b[0m /home/ga_aiot/anaconda3/envs/finrl/lib/python3.8/site-packages/gymnasium/spaces/box.py:227: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=392)\u001b[0m   logger.warn(\"Casting input x to numpy array.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Rwd:1513110.0308006166\n",
      "Checkpoint130 saved in directory model/ddpg_230407\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-07 23:43:03,163 E 17548 17548] (raylet) node_manager.cc:3040: 36 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "2023-04-07 23:43:26,347\tERROR actor_manager.py:496 -- Ray error, taking actor 6 out of service. The actor died unexpectedly before finishing this task.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=6915)\u001b[0m /home/ga_aiot/anaconda3/envs/finrl/lib/python3.8/site-packages/gymnasium/spaces/box.py:227: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=6915)\u001b[0m   logger.warn(\"Casting input x to numpy array.\")\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-07 23:44:03,168 E 17548 17548] (raylet) node_manager.cc:3040: 37 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-07 23:45:03,470 E 17548 17548] (raylet) node_manager.cc:3040: 41 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=27649)\u001b[0m /home/ga_aiot/anaconda3/envs/finrl/lib/python3.8/site-packages/gymnasium/spaces/box.py:227: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=27649)\u001b[0m   logger.warn(\"Casting input x to numpy array.\")\n",
      "2023-04-07 23:45:35,522\tERROR actor_manager.py:496 -- Ray error, taking actor 6 out of service. The actor died unexpectedly before finishing this task.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-07 23:46:03,472 E 17548 17548] (raylet) node_manager.cc:3040: 38 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=21525)\u001b[0m /home/ga_aiot/anaconda3/envs/finrl/lib/python3.8/site-packages/gymnasium/spaces/box.py:227: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=21525)\u001b[0m   logger.warn(\"Casting input x to numpy array.\")\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-07 23:47:03,473 E 17548 17548] (raylet) node_manager.cc:3040: 26 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Rwd:1513110.0308006166\n",
      "Checkpoint135 saved in directory model/ddpg_230407\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-07 23:48:03,474 E 17548 17548] (raylet) node_manager.cc:3040: 42 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-07 23:49:03,475 E 17548 17548] (raylet) node_manager.cc:3040: 41 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[36m(pid=5521)\u001b[0m Inconsistency detected by ld.so: ../elf/dl-tls.c: 517: _dl_allocate_tls_init: Assertion `listp != NULL' failed!\n",
      "2023-04-07 23:49:23,965\tWARNING worker.py:1866 -- A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: ffffffffffffffff39a79e424020edeae3d1a95601000000 Worker ID: af5c3232f0f2b3f4a2110e51b97e05bf4226db45b9a39d8b94468859 Node ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c Worker IP address: 172.21.0.25 Worker port: 42755 Worker PID: 5521 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-07 23:50:03,476 E 17548 17548] (raylet) node_manager.cc:3040: 41 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-07 23:51:03,477 E 17548 17548] (raylet) node_manager.cc:3040: 45 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-07 23:52:03,478 E 17548 17548] (raylet) node_manager.cc:3040: 58 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Rwd:1513110.0308006166\n",
      "Checkpoint140 saved in directory model/ddpg_230407\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-07 23:53:03,480 E 17548 17548] (raylet) node_manager.cc:3040: 56 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "2023-04-07 23:53:47,289\tERROR actor_manager.py:496 -- Ray error, taking actor 10 out of service. The actor died unexpectedly before finishing this task.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-07 23:54:03,481 E 17548 17548] (raylet) node_manager.cc:3040: 57 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=28900)\u001b[0m /home/ga_aiot/anaconda3/envs/finrl/lib/python3.8/site-packages/gymnasium/spaces/box.py:227: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=28900)\u001b[0m   logger.warn(\"Casting input x to numpy array.\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11683)\u001b[0m /home/ga_aiot/anaconda3/envs/finrl/lib/python3.8/site-packages/gymnasium/spaces/box.py:227: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11683)\u001b[0m   logger.warn(\"Casting input x to numpy array.\")\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-07 23:55:03,482 E 17548 17548] (raylet) node_manager.cc:3040: 40 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=21344)\u001b[0m /home/ga_aiot/anaconda3/envs/finrl/lib/python3.8/site-packages/gymnasium/spaces/box.py:227: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=21344)\u001b[0m   logger.warn(\"Casting input x to numpy array.\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11591)\u001b[0m /home/ga_aiot/anaconda3/envs/finrl/lib/python3.8/site-packages/gymnasium/spaces/box.py:227: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11591)\u001b[0m   logger.warn(\"Casting input x to numpy array.\")\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-07 23:56:03,743 E 17548 17548] (raylet) node_manager.cc:3040: 63 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "2023-04-07 23:56:15,204\tERROR actor_manager.py:496 -- Ray error, taking actor 9 out of service. The actor died unexpectedly before finishing this task.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=30056)\u001b[0m /home/ga_aiot/anaconda3/envs/finrl/lib/python3.8/site-packages/gymnasium/spaces/box.py:227: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=30056)\u001b[0m   logger.warn(\"Casting input x to numpy array.\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5237)\u001b[0m /home/ga_aiot/anaconda3/envs/finrl/lib/python3.8/site-packages/gymnasium/spaces/box.py:227: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5237)\u001b[0m   logger.warn(\"Casting input x to numpy array.\")\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-07 23:57:03,744 E 17548 17548] (raylet) node_manager.cc:3040: 60 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "2023-04-07 23:57:49,741\tERROR actor_manager.py:496 -- Ray error, taking actor 10 out of service. The actor died unexpectedly before finishing this task.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=13818)\u001b[0m /home/ga_aiot/anaconda3/envs/finrl/lib/python3.8/site-packages/gymnasium/spaces/box.py:227: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=13818)\u001b[0m   logger.warn(\"Casting input x to numpy array.\")\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-07 23:58:03,745 E 17548 17548] (raylet) node_manager.cc:3040: 35 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Rwd:1444818.258497869\n",
      "Checkpoint145 saved in directory model/ddpg_230407\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-07 23:59:03,769 E 17548 17548] (raylet) node_manager.cc:3040: 54 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "2023-04-07 23:59:10,892\tERROR actor_manager.py:496 -- Ray error, taking actor 6 out of service. The actor died unexpectedly before finishing this task.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=27138)\u001b[0m /home/ga_aiot/anaconda3/envs/finrl/lib/python3.8/site-packages/gymnasium/spaces/box.py:227: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=27138)\u001b[0m   logger.warn(\"Casting input x to numpy array.\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=29994)\u001b[0m /home/ga_aiot/anaconda3/envs/finrl/lib/python3.8/site-packages/gymnasium/spaces/box.py:227: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=29994)\u001b[0m   logger.warn(\"Casting input x to numpy array.\")\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 00:00:04,240 E 17548 17548] (raylet) node_manager.cc:3040: 56 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 00:01:04,240 E 17548 17548] (raylet) node_manager.cc:3040: 53 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=4216)\u001b[0m /home/ga_aiot/anaconda3/envs/finrl/lib/python3.8/site-packages/gymnasium/spaces/box.py:227: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=4216)\u001b[0m   logger.warn(\"Casting input x to numpy array.\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=25155)\u001b[0m /home/ga_aiot/anaconda3/envs/finrl/lib/python3.8/site-packages/gymnasium/spaces/box.py:227: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=25155)\u001b[0m   logger.warn(\"Casting input x to numpy array.\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1930)\u001b[0m /home/ga_aiot/anaconda3/envs/finrl/lib/python3.8/site-packages/gymnasium/spaces/box.py:227: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1930)\u001b[0m   logger.warn(\"Casting input x to numpy array.\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5400)\u001b[0m /home/ga_aiot/anaconda3/envs/finrl/lib/python3.8/site-packages/gymnasium/spaces/box.py:227: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5400)\u001b[0m   logger.warn(\"Casting input x to numpy array.\")\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 00:02:04,664 E 17548 17548] (raylet) node_manager.cc:3040: 63 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11187)\u001b[0m /home/ga_aiot/anaconda3/envs/finrl/lib/python3.8/site-packages/gymnasium/spaces/box.py:227: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11187)\u001b[0m   logger.warn(\"Casting input x to numpy array.\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15527)\u001b[0m /home/ga_aiot/anaconda3/envs/finrl/lib/python3.8/site-packages/gymnasium/spaces/box.py:227: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15527)\u001b[0m   logger.warn(\"Casting input x to numpy array.\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=26925)\u001b[0m /home/ga_aiot/anaconda3/envs/finrl/lib/python3.8/site-packages/gymnasium/spaces/box.py:227: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=26925)\u001b[0m   logger.warn(\"Casting input x to numpy array.\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=30317)\u001b[0m /home/ga_aiot/anaconda3/envs/finrl/lib/python3.8/site-packages/gymnasium/spaces/box.py:227: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=30317)\u001b[0m   logger.warn(\"Casting input x to numpy array.\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1981)\u001b[0m /home/ga_aiot/anaconda3/envs/finrl/lib/python3.8/site-packages/gymnasium/spaces/box.py:227: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1981)\u001b[0m   logger.warn(\"Casting input x to numpy array.\")\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 00:03:04,973 E 17548 17548] (raylet) node_manager.cc:3040: 67 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=13107)\u001b[0m /home/ga_aiot/anaconda3/envs/finrl/lib/python3.8/site-packages/gymnasium/spaces/box.py:227: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=13107)\u001b[0m   logger.warn(\"Casting input x to numpy array.\")\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 00:04:04,974 E 17548 17548] (raylet) node_manager.cc:3040: 67 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11647)\u001b[0m /home/ga_aiot/anaconda3/envs/finrl/lib/python3.8/site-packages/gymnasium/spaces/box.py:227: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11647)\u001b[0m   logger.warn(\"Casting input x to numpy array.\")\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 00:05:04,975 E 17548 17548] (raylet) node_manager.cc:3040: 72 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Rwd:1444818.258497869\n",
      "Checkpoint150 saved in directory model/ddpg_230407\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=31623)\u001b[0m /home/ga_aiot/anaconda3/envs/finrl/lib/python3.8/site-packages/gymnasium/spaces/box.py:227: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=31623)\u001b[0m   logger.warn(\"Casting input x to numpy array.\")\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 00:06:04,977 E 17548 17548] (raylet) node_manager.cc:3040: 60 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 00:07:04,978 E 17548 17548] (raylet) node_manager.cc:3040: 62 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 00:08:05,173 E 17548 17548] (raylet) node_manager.cc:3040: 76 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5617)\u001b[0m /home/ga_aiot/anaconda3/envs/finrl/lib/python3.8/site-packages/gymnasium/spaces/box.py:227: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5617)\u001b[0m   logger.warn(\"Casting input x to numpy array.\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=21181)\u001b[0m /home/ga_aiot/anaconda3/envs/finrl/lib/python3.8/site-packages/gymnasium/spaces/box.py:227: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=21181)\u001b[0m   logger.warn(\"Casting input x to numpy array.\")\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 00:09:05,174 E 17548 17548] (raylet) node_manager.cc:3040: 74 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11771)\u001b[0m /home/ga_aiot/anaconda3/envs/finrl/lib/python3.8/site-packages/gymnasium/spaces/box.py:227: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11771)\u001b[0m   logger.warn(\"Casting input x to numpy array.\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16063)\u001b[0m /home/ga_aiot/anaconda3/envs/finrl/lib/python3.8/site-packages/gymnasium/spaces/box.py:227: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16063)\u001b[0m   logger.warn(\"Casting input x to numpy array.\")\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 00:10:05,195 E 17548 17548] (raylet) node_manager.cc:3040: 73 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=26193)\u001b[0m /home/ga_aiot/anaconda3/envs/finrl/lib/python3.8/site-packages/gymnasium/spaces/box.py:227: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=26193)\u001b[0m   logger.warn(\"Casting input x to numpy array.\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=30186)\u001b[0m /home/ga_aiot/anaconda3/envs/finrl/lib/python3.8/site-packages/gymnasium/spaces/box.py:227: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=30186)\u001b[0m   logger.warn(\"Casting input x to numpy array.\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=7223)\u001b[0m /home/ga_aiot/anaconda3/envs/finrl/lib/python3.8/site-packages/gymnasium/spaces/box.py:227: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=7223)\u001b[0m   logger.warn(\"Casting input x to numpy array.\")\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 00:11:05,197 E 17548 17548] (raylet) node_manager.cc:3040: 72 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=22992)\u001b[0m /home/ga_aiot/anaconda3/envs/finrl/lib/python3.8/site-packages/gymnasium/spaces/box.py:227: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=22992)\u001b[0m   logger.warn(\"Casting input x to numpy array.\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=30449)\u001b[0m /home/ga_aiot/anaconda3/envs/finrl/lib/python3.8/site-packages/gymnasium/spaces/box.py:227: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=30449)\u001b[0m   logger.warn(\"Casting input x to numpy array.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Rwd:1444818.258497869\n",
      "Checkpoint155 saved in directory model/ddpg_230407\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 00:12:05,341 E 17548 17548] (raylet) node_manager.cc:3040: 71 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 00:13:05,342 E 17548 17548] (raylet) node_manager.cc:3040: 39 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=28244)\u001b[0m /home/ga_aiot/anaconda3/envs/finrl/lib/python3.8/site-packages/gymnasium/spaces/box.py:227: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=28244)\u001b[0m   logger.warn(\"Casting input x to numpy array.\")\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 00:14:05,614 E 17548 17548] (raylet) node_manager.cc:3040: 74 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=26310)\u001b[0m /home/ga_aiot/anaconda3/envs/finrl/lib/python3.8/site-packages/gymnasium/spaces/box.py:227: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=26310)\u001b[0m   logger.warn(\"Casting input x to numpy array.\")\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 00:15:05,615 E 17548 17548] (raylet) node_manager.cc:3040: 67 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 00:16:05,964 E 17548 17548] (raylet) node_manager.cc:3040: 30 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=18937)\u001b[0m day: 2353, episode: 10\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18937)\u001b[0m begin_total_asset: 1000000.00\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18937)\u001b[0m end_total_asset: 1852880.48\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18937)\u001b[0m total_reward: 852880.48\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18937)\u001b[0m total_cost: 999.00\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18937)\u001b[0m total_trades: 96473\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18937)\u001b[0m Sharpe: 0.439\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18937)\u001b[0m =================================\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18924)\u001b[0m day: 2353, episode: 10\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18924)\u001b[0m begin_total_asset: 1000000.00\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18924)\u001b[0m end_total_asset: 1772541.29\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18924)\u001b[0m total_reward: 772541.29\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18924)\u001b[0m total_cost: 999.00\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18924)\u001b[0m total_trades: 96473\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18924)\u001b[0m Sharpe: 0.423\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18924)\u001b[0m =================================\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18979)\u001b[0m day: 2353, episode: 10\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18979)\u001b[0m begin_total_asset: 1000000.00\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18979)\u001b[0m end_total_asset: 1858478.67\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18979)\u001b[0m total_reward: 858478.67\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18979)\u001b[0m total_cost: 999.00\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18979)\u001b[0m total_trades: 96473\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18979)\u001b[0m Sharpe: 0.439\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18979)\u001b[0m =================================\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18951)\u001b[0m day: 2353, episode: 10\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18951)\u001b[0m begin_total_asset: 1000000.00\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18951)\u001b[0m end_total_asset: 1866353.10\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18951)\u001b[0m total_reward: 866353.10\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18951)\u001b[0m total_cost: 999.00\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18951)\u001b[0m total_trades: 96473\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18951)\u001b[0m Sharpe: 0.439\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18951)\u001b[0m =================================\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18964)\u001b[0m day: 2353, episode: 10\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18964)\u001b[0m begin_total_asset: 1000000.00\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18964)\u001b[0m end_total_asset: 1906430.76\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18964)\u001b[0m total_reward: 906430.76\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18964)\u001b[0m total_cost: 999.00\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18964)\u001b[0m total_trades: 96473\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18964)\u001b[0m Sharpe: 0.450\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18964)\u001b[0m =================================\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19042)\u001b[0m day: 2353, episode: 10\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19042)\u001b[0m begin_total_asset: 1000000.00\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19042)\u001b[0m end_total_asset: 1857831.95\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19042)\u001b[0m total_reward: 857831.95\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19042)\u001b[0m total_cost: 999.00\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19042)\u001b[0m total_trades: 96473\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19042)\u001b[0m Sharpe: 0.440\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19042)\u001b[0m =================================\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19080)\u001b[0m day: 2353, episode: 10\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19080)\u001b[0m begin_total_asset: 1000000.00\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19080)\u001b[0m end_total_asset: 1928760.40\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19080)\u001b[0m total_reward: 928760.40\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19080)\u001b[0m total_cost: 999.00\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19080)\u001b[0m total_trades: 96473\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19080)\u001b[0m Sharpe: 0.453\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19080)\u001b[0m =================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=27287)\u001b[0m /home/ga_aiot/anaconda3/envs/finrl/lib/python3.8/site-packages/gymnasium/spaces/box.py:227: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=27287)\u001b[0m   logger.warn(\"Casting input x to numpy array.\")\n",
      "2023-04-08 00:16:58,538\tERROR actor_manager.py:496 -- Ray error, taking actor 6 out of service. The actor died unexpectedly before finishing this task.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1946)\u001b[0m /home/ga_aiot/anaconda3/envs/finrl/lib/python3.8/site-packages/gymnasium/spaces/box.py:227: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1946)\u001b[0m   logger.warn(\"Casting input x to numpy array.\")\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 00:17:06,096 E 17548 17548] (raylet) node_manager.cc:3040: 68 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=7021)\u001b[0m /home/ga_aiot/anaconda3/envs/finrl/lib/python3.8/site-packages/gymnasium/spaces/box.py:227: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=7021)\u001b[0m   logger.warn(\"Casting input x to numpy array.\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11271)\u001b[0m /home/ga_aiot/anaconda3/envs/finrl/lib/python3.8/site-packages/gymnasium/spaces/box.py:227: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11271)\u001b[0m   logger.warn(\"Casting input x to numpy array.\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20524)\u001b[0m /home/ga_aiot/anaconda3/envs/finrl/lib/python3.8/site-packages/gymnasium/spaces/box.py:227: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20524)\u001b[0m   logger.warn(\"Casting input x to numpy array.\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=26654)\u001b[0m /home/ga_aiot/anaconda3/envs/finrl/lib/python3.8/site-packages/gymnasium/spaces/box.py:227: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=26654)\u001b[0m   logger.warn(\"Casting input x to numpy array.\")\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 00:18:06,097 E 17548 17548] (raylet) node_manager.cc:3040: 67 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5911)\u001b[0m /home/ga_aiot/anaconda3/envs/finrl/lib/python3.8/site-packages/gymnasium/spaces/box.py:227: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5911)\u001b[0m   logger.warn(\"Casting input x to numpy array.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Rwd:1386964.2946080456\n",
      "Checkpoint160 saved in directory model/ddpg_230407\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-08 00:18:30,138\tERROR actor_manager.py:496 -- Ray error, taking actor 10 out of service. The actor died unexpectedly before finishing this task.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 00:19:06,226 E 17548 17548] (raylet) node_manager.cc:3040: 73 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 00:20:06,609 E 17548 17548] (raylet) node_manager.cc:3040: 71 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=28607)\u001b[0m /home/ga_aiot/anaconda3/envs/finrl/lib/python3.8/site-packages/gymnasium/spaces/box.py:227: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=28607)\u001b[0m   logger.warn(\"Casting input x to numpy array.\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=31994)\u001b[0m /home/ga_aiot/anaconda3/envs/finrl/lib/python3.8/site-packages/gymnasium/spaces/box.py:227: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=31994)\u001b[0m   logger.warn(\"Casting input x to numpy array.\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=6786)\u001b[0m /home/ga_aiot/anaconda3/envs/finrl/lib/python3.8/site-packages/gymnasium/spaces/box.py:227: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=6786)\u001b[0m   logger.warn(\"Casting input x to numpy array.\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=12346)\u001b[0m /home/ga_aiot/anaconda3/envs/finrl/lib/python3.8/site-packages/gymnasium/spaces/box.py:227: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=12346)\u001b[0m   logger.warn(\"Casting input x to numpy array.\")\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 00:21:06,880 E 17548 17548] (raylet) node_manager.cc:3040: 75 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=4163)\u001b[0m /home/ga_aiot/anaconda3/envs/finrl/lib/python3.8/site-packages/gymnasium/spaces/box.py:227: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=4163)\u001b[0m   logger.warn(\"Casting input x to numpy array.\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=12690)\u001b[0m /home/ga_aiot/anaconda3/envs/finrl/lib/python3.8/site-packages/gymnasium/spaces/box.py:227: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=12690)\u001b[0m   logger.warn(\"Casting input x to numpy array.\")\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 00:22:07,223 E 17548 17548] (raylet) node_manager.cc:3040: 82 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5791)\u001b[0m /home/ga_aiot/anaconda3/envs/finrl/lib/python3.8/site-packages/gymnasium/spaces/box.py:227: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5791)\u001b[0m   logger.warn(\"Casting input x to numpy array.\")\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 00:23:07,224 E 17548 17548] (raylet) node_manager.cc:3040: 77 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18905)\u001b[0m /home/ga_aiot/anaconda3/envs/finrl/lib/python3.8/site-packages/gymnasium/spaces/box.py:227: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18905)\u001b[0m   logger.warn(\"Casting input x to numpy array.\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=24323)\u001b[0m /home/ga_aiot/anaconda3/envs/finrl/lib/python3.8/site-packages/gymnasium/spaces/box.py:227: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=24323)\u001b[0m   logger.warn(\"Casting input x to numpy array.\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=28113)\u001b[0m /home/ga_aiot/anaconda3/envs/finrl/lib/python3.8/site-packages/gymnasium/spaces/box.py:227: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=28113)\u001b[0m   logger.warn(\"Casting input x to numpy array.\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2339)\u001b[0m /home/ga_aiot/anaconda3/envs/finrl/lib/python3.8/site-packages/gymnasium/spaces/box.py:227: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2339)\u001b[0m   logger.warn(\"Casting input x to numpy array.\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=6919)\u001b[0m /home/ga_aiot/anaconda3/envs/finrl/lib/python3.8/site-packages/gymnasium/spaces/box.py:227: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=6919)\u001b[0m   logger.warn(\"Casting input x to numpy array.\")\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 00:24:07,522 E 17548 17548] (raylet) node_manager.cc:3040: 70 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=21686)\u001b[0m /home/ga_aiot/anaconda3/envs/finrl/lib/python3.8/site-packages/gymnasium/spaces/box.py:227: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=21686)\u001b[0m   logger.warn(\"Casting input x to numpy array.\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5712)\u001b[0m /home/ga_aiot/anaconda3/envs/finrl/lib/python3.8/site-packages/gymnasium/spaces/box.py:227: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5712)\u001b[0m   logger.warn(\"Casting input x to numpy array.\")\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 00:25:07,523 E 17548 17548] (raylet) node_manager.cc:3040: 71 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=32206)\u001b[0m /home/ga_aiot/anaconda3/envs/finrl/lib/python3.8/site-packages/gymnasium/spaces/box.py:227: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=32206)\u001b[0m   logger.warn(\"Casting input x to numpy array.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Rwd:1386964.2946080456\n",
      "Checkpoint165 saved in directory model/ddpg_230407\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 00:26:07,524 E 17548 17548] (raylet) node_manager.cc:3040: 57 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=6184)\u001b[0m /home/ga_aiot/anaconda3/envs/finrl/lib/python3.8/site-packages/gymnasium/spaces/box.py:227: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=6184)\u001b[0m   logger.warn(\"Casting input x to numpy array.\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=9466)\u001b[0m /home/ga_aiot/anaconda3/envs/finrl/lib/python3.8/site-packages/gymnasium/spaces/box.py:227: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=9466)\u001b[0m   logger.warn(\"Casting input x to numpy array.\")\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 00:27:07,526 E 17548 17548] (raylet) node_manager.cc:3040: 78 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1706)\u001b[0m /home/ga_aiot/anaconda3/envs/finrl/lib/python3.8/site-packages/gymnasium/spaces/box.py:227: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1706)\u001b[0m   logger.warn(\"Casting input x to numpy array.\")\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 00:28:08,149 E 17548 17548] (raylet) node_manager.cc:3040: 71 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "2023-04-08 00:28:35,479\tERROR actor_manager.py:496 -- Ray error, taking actor 10 out of service. The actor died unexpectedly before finishing this task.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16073)\u001b[0m /home/ga_aiot/anaconda3/envs/finrl/lib/python3.8/site-packages/gymnasium/spaces/box.py:227: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16073)\u001b[0m   logger.warn(\"Casting input x to numpy array.\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=26610)\u001b[0m /home/ga_aiot/anaconda3/envs/finrl/lib/python3.8/site-packages/gymnasium/spaces/box.py:227: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=26610)\u001b[0m   logger.warn(\"Casting input x to numpy array.\")\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 00:29:08,150 E 17548 17548] (raylet) node_manager.cc:3040: 74 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "2023-04-08 00:29:43,777\tERROR actor_manager.py:496 -- Ray error, taking actor 9 out of service. The actor died unexpectedly before finishing this task.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16427)\u001b[0m /home/ga_aiot/anaconda3/envs/finrl/lib/python3.8/site-packages/gymnasium/spaces/box.py:227: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16427)\u001b[0m   logger.warn(\"Casting input x to numpy array.\")\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 00:30:08,548 E 17548 17548] (raylet) node_manager.cc:3040: 73 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 00:31:08,550 E 17548 17548] (raylet) node_manager.cc:3040: 74 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=32149)\u001b[0m /home/ga_aiot/anaconda3/envs/finrl/lib/python3.8/site-packages/gymnasium/spaces/box.py:227: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=32149)\u001b[0m   logger.warn(\"Casting input x to numpy array.\")\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 00:32:08,607 E 17548 17548] (raylet) node_manager.cc:3040: 78 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "2023-04-08 00:32:54,275\tERROR actor_manager.py:496 -- Ray error, taking actor 6 out of service. The actor died unexpectedly before finishing this task.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=7839)\u001b[0m /home/ga_aiot/anaconda3/envs/finrl/lib/python3.8/site-packages/gymnasium/spaces/box.py:227: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=7839)\u001b[0m   logger.warn(\"Casting input x to numpy array.\")\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 00:33:08,692 E 17548 17548] (raylet) node_manager.cc:3040: 77 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=30302)\u001b[0m /home/ga_aiot/anaconda3/envs/finrl/lib/python3.8/site-packages/gymnasium/spaces/box.py:227: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=30302)\u001b[0m   logger.warn(\"Casting input x to numpy array.\")\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 00:34:08,778 E 17548 17548] (raylet) node_manager.cc:3040: 80 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Rwd:1386964.2946080456\n",
      "Checkpoint170 saved in directory model/ddpg_230407\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=10859)\u001b[0m /home/ga_aiot/anaconda3/envs/finrl/lib/python3.8/site-packages/gymnasium/spaces/box.py:227: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=10859)\u001b[0m   logger.warn(\"Casting input x to numpy array.\")\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 00:35:08,779 E 17548 17548] (raylet) node_manager.cc:3040: 70 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=14409)\u001b[0m /home/ga_aiot/anaconda3/envs/finrl/lib/python3.8/site-packages/gymnasium/spaces/box.py:227: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=14409)\u001b[0m   logger.warn(\"Casting input x to numpy array.\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19264)\u001b[0m /home/ga_aiot/anaconda3/envs/finrl/lib/python3.8/site-packages/gymnasium/spaces/box.py:227: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19264)\u001b[0m   logger.warn(\"Casting input x to numpy array.\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=26614)\u001b[0m /home/ga_aiot/anaconda3/envs/finrl/lib/python3.8/site-packages/gymnasium/spaces/box.py:227: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=26614)\u001b[0m   logger.warn(\"Casting input x to numpy array.\")\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 00:36:08,867 E 17548 17548] (raylet) node_manager.cc:3040: 78 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 00:37:08,868 E 17548 17548] (raylet) node_manager.cc:3040: 71 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 00:38:08,870 E 17548 17548] (raylet) node_manager.cc:3040: 72 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=26525)\u001b[0m /home/ga_aiot/anaconda3/envs/finrl/lib/python3.8/site-packages/gymnasium/spaces/box.py:227: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=26525)\u001b[0m   logger.warn(\"Casting input x to numpy array.\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=31413)\u001b[0m /home/ga_aiot/anaconda3/envs/finrl/lib/python3.8/site-packages/gymnasium/spaces/box.py:227: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=31413)\u001b[0m   logger.warn(\"Casting input x to numpy array.\")\n",
      "2023-04-08 00:38:51,803\tWARNING worker.py:1866 -- A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: ffffffffffffffffe636fa93420a5078ad13feb401000000 Worker ID: e3f1ebc0e13d68c4d3b3d13aa257152f959124b6c932fbcfd9e548fb Node ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c Worker IP address: 172.21.0.25 Worker port: 39655 Worker PID: 11807 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\u001b[2m\u001b[36m(pid=11807)\u001b[0m Inconsistency detected by ld.so: ../elf/dl-tls.c: 517: _dl_allocate_tls_init: Assertion `listp != NULL' failed!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=17034)\u001b[0m /home/ga_aiot/anaconda3/envs/finrl/lib/python3.8/site-packages/gymnasium/spaces/box.py:227: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=17034)\u001b[0m   logger.warn(\"Casting input x to numpy array.\")\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 00:39:09,024 E 17548 17548] (raylet) node_manager.cc:3040: 76 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 00:40:09,026 E 17548 17548] (raylet) node_manager.cc:3040: 37 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=28433)\u001b[0m /home/ga_aiot/anaconda3/envs/finrl/lib/python3.8/site-packages/gymnasium/spaces/box.py:227: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=28433)\u001b[0m   logger.warn(\"Casting input x to numpy array.\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2147)\u001b[0m /home/ga_aiot/anaconda3/envs/finrl/lib/python3.8/site-packages/gymnasium/spaces/box.py:227: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2147)\u001b[0m   logger.warn(\"Casting input x to numpy array.\")\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 00:41:09,164 E 17548 17548] (raylet) node_manager.cc:3040: 50 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Rwd:1386964.2946080456\n",
      "Checkpoint175 saved in directory model/ddpg_230407\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 00:42:09,430 E 17548 17548] (raylet) node_manager.cc:3040: 80 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 00:43:09,718 E 17548 17548] (raylet) node_manager.cc:3040: 78 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 00:44:09,727 E 17548 17548] (raylet) node_manager.cc:3040: 77 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "2023-04-08 00:44:12,730\tERROR actor_manager.py:496 -- Ray error, taking actor 6 out of service. The actor died unexpectedly before finishing this task.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=6570)\u001b[0m /home/ga_aiot/anaconda3/envs/finrl/lib/python3.8/site-packages/gymnasium/spaces/box.py:227: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=6570)\u001b[0m   logger.warn(\"Casting input x to numpy array.\")\n",
      "\u001b[2m\u001b[36m(pid=21788)\u001b[0m [2023-04-08 00:44:40,569 E 21788 22216] core_worker.h:1360: Mismatched WorkerID: ignoring RPC for previous worker 2f2b225d4364b83f6e5b3f8d5d1b00a802a85294cd3fd134ecd6a487, current worker ID: c8744d4474a4786547256b3ae13cfcd2f1bc73f27a17e54ea3200d35\n",
      "\u001b[2m\u001b[36m(pid=21788)\u001b[0m [2023-04-08 00:44:40,771 E 21788 22216] core_worker.h:1360: Mismatched WorkerID: ignoring RPC for previous worker 2f2b225d4364b83f6e5b3f8d5d1b00a802a85294cd3fd134ecd6a487, current worker ID: c8744d4474a4786547256b3ae13cfcd2f1bc73f27a17e54ea3200d35\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 00:45:09,728 E 17548 17548] (raylet) node_manager.cc:3040: 68 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "2023-04-08 00:45:56,431\tERROR actor_manager.py:496 -- Ray error, taking actor 9 out of service. The actor died unexpectedly before finishing this task.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=24443)\u001b[0m /home/ga_aiot/anaconda3/envs/finrl/lib/python3.8/site-packages/gymnasium/spaces/box.py:227: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=24443)\u001b[0m   logger.warn(\"Casting input x to numpy array.\")\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 00:46:09,730 E 17548 17548] (raylet) node_manager.cc:3040: 76 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "2023-04-08 00:46:32,555\tERROR actor_manager.py:496 -- Ray error, taking actor 10 out of service. The actor died unexpectedly before finishing this task.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=7223)\u001b[0m /home/ga_aiot/anaconda3/envs/finrl/lib/python3.8/site-packages/gymnasium/spaces/box.py:227: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=7223)\u001b[0m   logger.warn(\"Casting input x to numpy array.\")\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 00:47:09,923 E 17548 17548] (raylet) node_manager.cc:3040: 77 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 00:48:10,047 E 17548 17548] (raylet) node_manager.cc:3040: 67 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "2023-04-08 00:48:16,932\tERROR actor_manager.py:496 -- Ray error, taking actor 6 out of service. The actor died unexpectedly before finishing this task.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=31209)\u001b[0m /home/ga_aiot/anaconda3/envs/finrl/lib/python3.8/site-packages/gymnasium/spaces/box.py:227: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=31209)\u001b[0m   logger.warn(\"Casting input x to numpy array.\")\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 00:49:10,084 E 17548 17548] (raylet) node_manager.cc:3040: 71 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Rwd:1344181.9610400135\n",
      "Checkpoint180 saved in directory model/ddpg_230407\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-08 00:49:51,895\tERROR actor_manager.py:496 -- Ray error, taking actor 10 out of service. The actor died unexpectedly before finishing this task.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 00:50:10,086 E 17548 17548] (raylet) node_manager.cc:3040: 68 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3185)\u001b[0m /home/ga_aiot/anaconda3/envs/finrl/lib/python3.8/site-packages/gymnasium/spaces/box.py:227: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3185)\u001b[0m   logger.warn(\"Casting input x to numpy array.\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=31225)\u001b[0m /home/ga_aiot/anaconda3/envs/finrl/lib/python3.8/site-packages/gymnasium/spaces/box.py:227: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=31225)\u001b[0m   logger.warn(\"Casting input x to numpy array.\")\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 00:51:10,256 E 17548 17548] (raylet) node_manager.cc:3040: 72 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 00:52:10,397 E 17548 17548] (raylet) node_manager.cc:3040: 78 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2673)\u001b[0m /home/ga_aiot/anaconda3/envs/finrl/lib/python3.8/site-packages/gymnasium/spaces/box.py:227: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2673)\u001b[0m   logger.warn(\"Casting input x to numpy array.\")\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 00:53:10,721 E 17548 17548] (raylet) node_manager.cc:3040: 77 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "2023-04-08 00:53:52,292\tERROR actor_manager.py:496 -- Ray error, taking actor 6 out of service. The actor died unexpectedly before finishing this task.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16740)\u001b[0m /home/ga_aiot/anaconda3/envs/finrl/lib/python3.8/site-packages/gymnasium/spaces/box.py:227: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16740)\u001b[0m   logger.warn(\"Casting input x to numpy array.\")\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 00:54:10,866 E 17548 17548] (raylet) node_manager.cc:3040: 76 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=27212)\u001b[0m /home/ga_aiot/anaconda3/envs/finrl/lib/python3.8/site-packages/gymnasium/spaces/box.py:227: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=27212)\u001b[0m   logger.warn(\"Casting input x to numpy array.\")\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 00:55:10,868 E 17548 17548] (raylet) node_manager.cc:3040: 74 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "2023-04-08 00:55:12,102\tERROR actor_manager.py:496 -- Ray error, taking actor 10 out of service. The actor died unexpectedly before finishing this task.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20745)\u001b[0m /home/ga_aiot/anaconda3/envs/finrl/lib/python3.8/site-packages/gymnasium/spaces/box.py:227: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20745)\u001b[0m   logger.warn(\"Casting input x to numpy array.\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=29080)\u001b[0m /home/ga_aiot/anaconda3/envs/finrl/lib/python3.8/site-packages/gymnasium/spaces/box.py:227: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=29080)\u001b[0m   logger.warn(\"Casting input x to numpy array.\")\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 00:56:10,870 E 17548 17548] (raylet) node_manager.cc:3040: 75 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "2023-04-08 00:56:46,846\tERROR actor_manager.py:496 -- Ray error, taking actor 10 out of service. The actor died unexpectedly before finishing this task.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=30379)\u001b[0m /home/ga_aiot/anaconda3/envs/finrl/lib/python3.8/site-packages/gymnasium/spaces/box.py:227: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=30379)\u001b[0m   logger.warn(\"Casting input x to numpy array.\")\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 00:57:11,142 E 17548 17548] (raylet) node_manager.cc:3040: 79 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Rwd:1344181.9610400135\n",
      "Checkpoint185 saved in directory model/ddpg_230407\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 00:58:11,447 E 17548 17548] (raylet) node_manager.cc:3040: 80 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 00:59:11,672 E 17548 17548] (raylet) node_manager.cc:3040: 77 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 01:00:11,673 E 17548 17548] (raylet) node_manager.cc:3040: 61 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 01:01:11,989 E 17548 17548] (raylet) node_manager.cc:3040: 63 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 01:02:12,352 E 17548 17548] (raylet) node_manager.cc:3040: 77 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 01:03:12,353 E 17548 17548] (raylet) node_manager.cc:3040: 77 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "2023-04-08 01:04:06,841\tERROR actor_manager.py:496 -- Ray error, taking actor 6 out of service. The actor died unexpectedly before finishing this task.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 01:04:12,416 E 17548 17548] (raylet) node_manager.cc:3040: 73 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Rwd:1344181.9610400135\n",
      "Checkpoint190 saved in directory model/ddpg_230407\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=11977)\u001b[0m /home/ga_aiot/anaconda3/envs/finrl/lib/python3.8/site-packages/gymnasium/spaces/box.py:227: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11977)\u001b[0m   logger.warn(\"Casting input x to numpy array.\")\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 01:05:12,879 E 17548 17548] (raylet) node_manager.cc:3040: 27 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 01:06:12,882 E 17548 17548] (raylet) node_manager.cc:3040: 77 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 01:07:12,884 E 17548 17548] (raylet) node_manager.cc:3040: 78 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 01:08:12,885 E 17548 17548] (raylet) node_manager.cc:3040: 78 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 01:09:13,236 E 17548 17548] (raylet) node_manager.cc:3040: 78 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "2023-04-08 01:09:59,547\tERROR actor_manager.py:496 -- Ray error, taking actor 10 out of service. The actor died unexpectedly before finishing this task.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 01:10:13,444 E 17548 17548] (raylet) node_manager.cc:3040: 68 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19969)\u001b[0m /home/ga_aiot/anaconda3/envs/finrl/lib/python3.8/site-packages/gymnasium/spaces/box.py:227: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19969)\u001b[0m   logger.warn(\"Casting input x to numpy array.\")\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 01:11:13,986 E 17548 17548] (raylet) node_manager.cc:3040: 45 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "2023-04-08 01:11:47,221\tERROR actor_manager.py:496 -- Ray error, taking actor 6 out of service. The actor died unexpectedly before finishing this task.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=6020)\u001b[0m /home/ga_aiot/anaconda3/envs/finrl/lib/python3.8/site-packages/gymnasium/spaces/box.py:227: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=6020)\u001b[0m   logger.warn(\"Casting input x to numpy array.\")\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 01:12:14,352 E 17548 17548] (raylet) node_manager.cc:3040: 75 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Rwd:1309387.371752837\n",
      "Checkpoint195 saved in directory model/ddpg_230407\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 01:13:14,420 E 17548 17548] (raylet) node_manager.cc:3040: 78 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 01:14:14,521 E 17548 17548] (raylet) node_manager.cc:3040: 77 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 01:15:14,957 E 17548 17548] (raylet) node_manager.cc:3040: 77 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 01:16:15,175 E 17548 17548] (raylet) node_manager.cc:3040: 77 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 01:17:15,654 E 17548 17548] (raylet) node_manager.cc:3040: 78 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "2023-04-08 01:17:22,743\tERROR actor_manager.py:496 -- Ray error, taking actor 10 out of service. The actor died unexpectedly before finishing this task.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=29316)\u001b[0m /home/ga_aiot/anaconda3/envs/finrl/lib/python3.8/site-packages/gymnasium/spaces/box.py:227: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=29316)\u001b[0m   logger.warn(\"Casting input x to numpy array.\")\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 01:18:15,655 E 17548 17548] (raylet) node_manager.cc:3040: 76 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 01:19:15,784 E 17548 17548] (raylet) node_manager.cc:3040: 78 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 01:20:16,293 E 17548 17548] (raylet) node_manager.cc:3040: 78 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 01:21:16,912 E 17548 17548] (raylet) node_manager.cc:3040: 78 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Rwd:1309387.371752837\n",
      "Checkpoint200 saved in directory model/ddpg_230407\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 01:22:17,281 E 17548 17548] (raylet) node_manager.cc:3040: 79 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 01:23:17,586 E 17548 17548] (raylet) node_manager.cc:3040: 81 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 01:24:17,587 E 17548 17548] (raylet) node_manager.cc:3040: 85 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 01:25:17,589 E 17548 17548] (raylet) node_manager.cc:3040: 86 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 01:26:17,593 E 17548 17548] (raylet) node_manager.cc:3040: 86 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 01:27:18,022 E 17548 17548] (raylet) node_manager.cc:3040: 86 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 01:28:18,478 E 17548 17548] (raylet) node_manager.cc:3040: 87 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 01:29:18,705 E 17548 17548] (raylet) node_manager.cc:3040: 85 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 01:30:18,760 E 17548 17548] (raylet) node_manager.cc:3040: 88 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 01:31:19,192 E 17548 17548] (raylet) node_manager.cc:3040: 87 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 01:32:19,265 E 17548 17548] (raylet) node_manager.cc:3040: 87 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 01:33:19,321 E 17548 17548] (raylet) node_manager.cc:3040: 88 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 01:34:19,372 E 17548 17548] (raylet) node_manager.cc:3040: 86 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 01:35:19,518 E 17548 17548] (raylet) node_manager.cc:3040: 87 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 01:36:19,879 E 17548 17548] (raylet) node_manager.cc:3040: 87 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 01:37:19,997 E 17548 17548] (raylet) node_manager.cc:3040: 88 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 01:38:19,998 E 17548 17548] (raylet) node_manager.cc:3040: 87 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 01:39:20,162 E 17548 17548] (raylet) node_manager.cc:3040: 88 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 01:40:20,163 E 17548 17548] (raylet) node_manager.cc:3040: 85 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 01:41:20,164 E 17548 17548] (raylet) node_manager.cc:3040: 86 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 01:42:20,165 E 17548 17548] (raylet) node_manager.cc:3040: 87 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 01:43:20,310 E 17548 17548] (raylet) node_manager.cc:3040: 87 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 01:44:20,311 E 17548 17548] (raylet) node_manager.cc:3040: 86 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 01:45:20,533 E 17548 17548] (raylet) node_manager.cc:3040: 86 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 01:46:20,535 E 17548 17548] (raylet) node_manager.cc:3040: 87 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 01:47:20,935 E 17548 17548] (raylet) node_manager.cc:3040: 88 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 01:48:20,936 E 17548 17548] (raylet) node_manager.cc:3040: 86 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 01:49:20,937 E 17548 17548] (raylet) node_manager.cc:3040: 86 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 01:50:20,939 E 17548 17548] (raylet) node_manager.cc:3040: 87 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[36m(pid=31981)\u001b[0m [2023-04-08 01:51:01,532 E 31981 32253] core_worker.h:1360: Mismatched WorkerID: ignoring RPC for previous worker 3bf7e1fbe3215565df0c342c02dd278aa8a56b0e19c9593b7fbc559a, current worker ID: 8d69cd93373825997a96f48267f0e5d185279af13673b245caf07e20\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 01:51:21,295 E 17548 17548] (raylet) node_manager.cc:3040: 88 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 01:52:21,296 E 17548 17548] (raylet) node_manager.cc:3040: 87 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 01:53:21,297 E 17548 17548] (raylet) node_manager.cc:3040: 87 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 01:54:21,298 E 17548 17548] (raylet) node_manager.cc:3040: 87 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 01:55:21,393 E 17548 17548] (raylet) node_manager.cc:3040: 87 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 01:56:21,394 E 17548 17548] (raylet) node_manager.cc:3040: 88 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 01:57:21,396 E 17548 17548] (raylet) node_manager.cc:3040: 87 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 01:58:21,713 E 17548 17548] (raylet) node_manager.cc:3040: 89 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 01:59:21,951 E 17548 17548] (raylet) node_manager.cc:3040: 89 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 02:00:21,952 E 17548 17548] (raylet) node_manager.cc:3040: 87 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 02:01:22,145 E 17548 17548] (raylet) node_manager.cc:3040: 89 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 02:02:22,147 E 17548 17548] (raylet) node_manager.cc:3040: 88 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 02:03:22,543 E 17548 17548] (raylet) node_manager.cc:3040: 90 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 02:04:22,794 E 17548 17548] (raylet) node_manager.cc:3040: 88 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 02:05:22,967 E 17548 17548] (raylet) node_manager.cc:3040: 87 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 02:06:22,968 E 17548 17548] (raylet) node_manager.cc:3040: 87 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 02:07:23,075 E 17548 17548] (raylet) node_manager.cc:3040: 90 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 02:08:23,341 E 17548 17548] (raylet) node_manager.cc:3040: 89 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 02:09:23,531 E 17548 17548] (raylet) node_manager.cc:3040: 88 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 02:10:23,541 E 17548 17548] (raylet) node_manager.cc:3040: 89 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 02:11:23,635 E 17548 17548] (raylet) node_manager.cc:3040: 89 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 02:12:23,736 E 17548 17548] (raylet) node_manager.cc:3040: 89 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 02:13:23,971 E 17548 17548] (raylet) node_manager.cc:3040: 89 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 02:14:24,012 E 17548 17548] (raylet) node_manager.cc:3040: 87 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 02:15:24,045 E 17548 17548] (raylet) node_manager.cc:3040: 87 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 02:16:24,046 E 17548 17548] (raylet) node_manager.cc:3040: 88 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 02:17:24,048 E 17548 17548] (raylet) node_manager.cc:3040: 88 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 02:18:24,236 E 17548 17548] (raylet) node_manager.cc:3040: 88 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 02:19:24,500 E 17548 17548] (raylet) node_manager.cc:3040: 87 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 02:20:24,859 E 17548 17548] (raylet) node_manager.cc:3040: 88 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 02:21:24,873 E 17548 17548] (raylet) node_manager.cc:3040: 88 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 02:22:24,874 E 17548 17548] (raylet) node_manager.cc:3040: 87 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 02:23:25,182 E 17548 17548] (raylet) node_manager.cc:3040: 88 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 02:24:25,320 E 17548 17548] (raylet) node_manager.cc:3040: 87 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 02:25:25,444 E 17548 17548] (raylet) node_manager.cc:3040: 86 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 02:26:25,592 E 17548 17548] (raylet) node_manager.cc:3040: 87 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 02:27:25,692 E 17548 17548] (raylet) node_manager.cc:3040: 86 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 02:28:25,693 E 17548 17548] (raylet) node_manager.cc:3040: 86 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 02:29:25,694 E 17548 17548] (raylet) node_manager.cc:3040: 86 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 02:30:26,123 E 17548 17548] (raylet) node_manager.cc:3040: 87 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 02:31:26,480 E 17548 17548] (raylet) node_manager.cc:3040: 87 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 02:32:26,482 E 17548 17548] (raylet) node_manager.cc:3040: 85 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 02:33:26,483 E 17548 17548] (raylet) node_manager.cc:3040: 86 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 02:34:26,542 E 17548 17548] (raylet) node_manager.cc:3040: 85 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 02:35:26,544 E 17548 17548] (raylet) node_manager.cc:3040: 86 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 02:36:26,900 E 17548 17548] (raylet) node_manager.cc:3040: 87 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 02:37:27,152 E 17548 17548] (raylet) node_manager.cc:3040: 86 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 02:38:27,340 E 17548 17548] (raylet) node_manager.cc:3040: 86 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 02:39:27,659 E 17548 17548] (raylet) node_manager.cc:3040: 87 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 02:40:28,137 E 17548 17548] (raylet) node_manager.cc:3040: 84 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 02:41:28,138 E 17548 17548] (raylet) node_manager.cc:3040: 86 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 02:42:28,482 E 17548 17548] (raylet) node_manager.cc:3040: 88 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 02:43:28,483 E 17548 17548] (raylet) node_manager.cc:3040: 86 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 02:44:28,711 E 17548 17548] (raylet) node_manager.cc:3040: 86 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 02:45:28,712 E 17548 17548] (raylet) node_manager.cc:3040: 85 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 02:46:28,713 E 17548 17548] (raylet) node_manager.cc:3040: 87 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 02:47:28,751 E 17548 17548] (raylet) node_manager.cc:3040: 86 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 02:48:28,752 E 17548 17548] (raylet) node_manager.cc:3040: 86 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 02:49:28,835 E 17548 17548] (raylet) node_manager.cc:3040: 86 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 02:50:29,129 E 17548 17548] (raylet) node_manager.cc:3040: 87 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 02:51:29,539 E 17548 17548] (raylet) node_manager.cc:3040: 87 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 02:52:29,924 E 17548 17548] (raylet) node_manager.cc:3040: 87 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 02:53:29,925 E 17548 17548] (raylet) node_manager.cc:3040: 86 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 02:54:29,926 E 17548 17548] (raylet) node_manager.cc:3040: 64 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 02:55:29,927 E 17548 17548] (raylet) node_manager.cc:3040: 56 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 02:56:29,929 E 17548 17548] (raylet) node_manager.cc:3040: 87 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 02:57:29,930 E 17548 17548] (raylet) node_manager.cc:3040: 85 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 02:58:30,090 E 17548 17548] (raylet) node_manager.cc:3040: 88 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 02:59:30,091 E 17548 17548] (raylet) node_manager.cc:3040: 87 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=31039)\u001b[0m /home/ga_aiot/anaconda3/envs/finrl/lib/python3.8/site-packages/gymnasium/spaces/box.py:227: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=31039)\u001b[0m   logger.warn(\"Casting input x to numpy array.\")\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 03:00:30,092 E 17548 17548] (raylet) node_manager.cc:3040: 51 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 03:01:30,094 E 17548 17548] (raylet) node_manager.cc:3040: 86 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 03:02:30,482 E 17548 17548] (raylet) node_manager.cc:3040: 87 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 03:03:30,483 E 17548 17548] (raylet) node_manager.cc:3040: 87 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 03:04:30,770 E 17548 17548] (raylet) node_manager.cc:3040: 86 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 03:05:30,841 E 17548 17548] (raylet) node_manager.cc:3040: 86 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 03:06:31,065 E 17548 17548] (raylet) node_manager.cc:3040: 85 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 03:07:31,066 E 17548 17548] (raylet) node_manager.cc:3040: 87 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 03:08:31,393 E 17548 17548] (raylet) node_manager.cc:3040: 87 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 03:09:31,459 E 17548 17548] (raylet) node_manager.cc:3040: 87 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 03:10:31,460 E 17548 17548] (raylet) node_manager.cc:3040: 87 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 03:11:31,461 E 17548 17548] (raylet) node_manager.cc:3040: 86 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 03:12:31,475 E 17548 17548] (raylet) node_manager.cc:3040: 87 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 03:13:31,815 E 17548 17548] (raylet) node_manager.cc:3040: 87 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 03:14:32,134 E 17548 17548] (raylet) node_manager.cc:3040: 88 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 03:15:32,504 E 17548 17548] (raylet) node_manager.cc:3040: 87 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 03:16:32,505 E 17548 17548] (raylet) node_manager.cc:3040: 88 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 03:17:32,757 E 17548 17548] (raylet) node_manager.cc:3040: 88 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 03:18:33,014 E 17548 17548] (raylet) node_manager.cc:3040: 88 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[36m(pid=13150)\u001b[0m [2023-04-08 03:19:21,927 E 13150 13402] core_worker.h:1360: Mismatched WorkerID: ignoring RPC for previous worker 5e985b8e45fd19abc056786c31edd666bf3abf236220fdaef0b337bd, current worker ID: 5b42cf26272eab94e6c4e5a7dcd752996b3a6f3c97ece13d5b4b78ae\n",
      "\u001b[2m\u001b[36m(pid=13150)\u001b[0m [2023-04-08 03:19:22,129 E 13150 13402] core_worker.h:1360: Mismatched WorkerID: ignoring RPC for previous worker 5e985b8e45fd19abc056786c31edd666bf3abf236220fdaef0b337bd, current worker ID: 5b42cf26272eab94e6c4e5a7dcd752996b3a6f3c97ece13d5b4b78ae\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 03:19:33,277 E 17548 17548] (raylet) node_manager.cc:3040: 87 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 03:20:33,382 E 17548 17548] (raylet) node_manager.cc:3040: 86 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 03:21:33,741 E 17548 17548] (raylet) node_manager.cc:3040: 88 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 03:22:34,105 E 17548 17548] (raylet) node_manager.cc:3040: 87 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 03:23:34,362 E 17548 17548] (raylet) node_manager.cc:3040: 87 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 03:24:34,363 E 17548 17548] (raylet) node_manager.cc:3040: 87 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 03:25:34,469 E 17548 17548] (raylet) node_manager.cc:3040: 86 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 03:26:34,471 E 17548 17548] (raylet) node_manager.cc:3040: 87 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 03:27:34,472 E 17548 17548] (raylet) node_manager.cc:3040: 87 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 03:28:34,800 E 17548 17548] (raylet) node_manager.cc:3040: 87 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 03:29:34,801 E 17548 17548] (raylet) node_manager.cc:3040: 88 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 03:30:35,069 E 17548 17548] (raylet) node_manager.cc:3040: 88 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 03:31:35,070 E 17548 17548] (raylet) node_manager.cc:3040: 88 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 03:32:35,076 E 17548 17548] (raylet) node_manager.cc:3040: 87 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 03:33:35,183 E 17548 17548] (raylet) node_manager.cc:3040: 88 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 03:34:35,184 E 17548 17548] (raylet) node_manager.cc:3040: 87 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 03:35:35,459 E 17548 17548] (raylet) node_manager.cc:3040: 86 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 03:36:35,460 E 17548 17548] (raylet) node_manager.cc:3040: 88 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 03:37:35,734 E 17548 17548] (raylet) node_manager.cc:3040: 80 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 03:38:35,931 E 17548 17548] (raylet) node_manager.cc:3040: 89 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 03:39:35,933 E 17548 17548] (raylet) node_manager.cc:3040: 87 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 03:40:36,254 E 17548 17548] (raylet) node_manager.cc:3040: 78 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "# Train away -------------------------------------------------------------\n",
    "total_episodes = 200\n",
    "agent_name = 'td3'\n",
    "ep = 0\n",
    "results = []\n",
    "bar = tqdm(total=total_episodes, desc=\"Episodes\")\n",
    "date = datetime.now().strftime('%y%m%d')\n",
    "\n",
    "while ep <= total_episodes:\n",
    "    results.append(trainer.train())\n",
    "    ep += 1\n",
    "    bar.update(n=1)\n",
    "    rwd = results[-1]['episode_reward_mean']\n",
    "    if ep % 20 == 0:\n",
    "        print(f'Mean Rwd:{rwd}')\n",
    "    if ep % 100 == 0:\n",
    "        #cwd_checkpoint = \"results/checkpoints/\" +  + '_' + str(ep)\n",
    "        cwd_checkpoint = f\"model/{agent_name}_{date}\"\n",
    "        trainer.save(cwd_checkpoint)\n",
    "        print(f\"Checkpoint{ep} saved in directory {cwd_checkpoint}\")\n",
    "bar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e1b82920-0746-430c-8c59-5c3467ebd07e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "passed model weights\n",
      "config created\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ga_aiot/anaconda3/envs/finrl/lib/python3.8/site-packages/gymnasium/spaces/box.py:227: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "  logger.warn(\"Casting input x to numpy array.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Weights loaded. \n",
      "zip_file created.\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'file zipped at ckpt_wt230407_201.zip'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 20\u001b[0m\n\u001b[1;32m     18\u001b[0m zipped_filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mckpt_wt\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.zip\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     19\u001b[0m zipped \u001b[38;5;241m=\u001b[39m zipfilem(ckpt2,zipped_filename)\n\u001b[0;32m---> 20\u001b[0m st \u001b[38;5;241m=\u001b[39m \u001b[43msendfile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mzipped\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfile\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mzipped\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mckpt2\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ; \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mst\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/findrl_ray/finenv/save_model.py:40\u001b[0m, in \u001b[0;36msendfile\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msendfile\u001b[39m(filename):\n\u001b[1;32m     39\u001b[0m     zip_filename \u001b[38;5;241m=\u001b[39m filename\n\u001b[0;32m---> 40\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mzip_filename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:ftp\u001b[38;5;241m.\u001b[39mstorbinary(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSTOR \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m zip_filename, f)\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;66;03m# Close the FTP connection\u001b[39;00m\n\u001b[1;32m     42\u001b[0m     f\u001b[38;5;241m.\u001b[39mclose()\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'file zipped at ckpt_wt230407_201.zip'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-08 03:49:38,011 E 17548 17548] (raylet) node_manager.cc:3040: 75 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c50d046abda4a2ab6ab4bafb37b97be86733da4e69c0b14606e92e5c, IP: 172.21.0.25) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.25`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
     ]
    }
   ],
   "source": [
    "#Save latest ckpt point\n",
    "cwd_checkpoint = f\"results/org_{agent_name}_{date}_{ep}\"\n",
    "trainer.save(cwd_checkpoint)\n",
    "#Extract model weights \n",
    "model_weights = trainer.get_policy().get_weights()\n",
    "print('passed model weights')\n",
    "config2 = TD3Config()\n",
    "print('config created')\n",
    "config2 = config2.environment(env_config={'hmax':500,'initial_amount':1000000})  \n",
    "config2 = config2.rollouts(num_rollout_workers=0) \n",
    "config2 = config2.framework(framework=\"torch\")\n",
    "config2[\"model\"][\"fcnet_hiddens\"] = [256, 256, 256]\n",
    "trainer2 = config2.build(env=\"finrl\") \n",
    "trainer2.get_policy().set_weights(model_weights)\n",
    "print('New Weights loaded. ')\n",
    "ckpt2 = f\"{cwd_checkpoint}_wt\"\n",
    "trainer2.save(ckpt2)\n",
    "zipped_filename = f'ckpt_wt{date}_{ep}.zip'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59824806-2a79-4959-b9a1-4a8074c12015",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'sendall'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#zipped = zipfilem(ckpt2,zipped_filename)\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m st \u001b[38;5;241m=\u001b[39m \u001b[43msendfile\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mckpt_wt230407_201.zip\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfile from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mckpt2\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ; \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mst\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/findrl_ray/finenv/save_model.py:39\u001b[0m, in \u001b[0;36msendfile\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msendfile\u001b[39m(filename):\n\u001b[0;32m---> 39\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(filename, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\u001b[43mftp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstorbinary\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mSTOR \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;66;03m# Close the FTP connection\u001b[39;00m\n\u001b[1;32m     41\u001b[0m     f\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/anaconda3/envs/finrl/lib/python3.8/ftplib.py:493\u001b[0m, in \u001b[0;36mFTP.storbinary\u001b[0;34m(self, cmd, fp, blocksize, callback, rest)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstorbinary\u001b[39m(\u001b[38;5;28mself\u001b[39m, cmd, fp, blocksize\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8192\u001b[39m, callback\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, rest\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    479\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Store a file in binary mode.  A new port is created for you.\u001b[39;00m\n\u001b[1;32m    480\u001b[0m \n\u001b[1;32m    481\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    491\u001b[0m \u001b[38;5;124;03m      The response code.\u001b[39;00m\n\u001b[1;32m    492\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 493\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvoidcmd\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mTYPE I\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    494\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransfercmd(cmd, rest) \u001b[38;5;28;01mas\u001b[39;00m conn:\n\u001b[1;32m    495\u001b[0m         \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/finrl/lib/python3.8/ftplib.py:281\u001b[0m, in \u001b[0;36mFTP.voidcmd\u001b[0;34m(self, cmd)\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvoidcmd\u001b[39m(\u001b[38;5;28mself\u001b[39m, cmd):\n\u001b[1;32m    280\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Send a command and expect a response beginning with '2'.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 281\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mputcmd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcmd\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    282\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvoidresp()\n",
      "File \u001b[0;32m~/anaconda3/envs/finrl/lib/python3.8/ftplib.py:203\u001b[0m, in \u001b[0;36mFTP.putcmd\u001b[0;34m(self, line)\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mputcmd\u001b[39m(\u001b[38;5;28mself\u001b[39m, line):\n\u001b[1;32m    202\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdebugging: \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m*cmd*\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msanitize(line))\n\u001b[0;32m--> 203\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mputline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mline\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/finrl/lib/python3.8/ftplib.py:198\u001b[0m, in \u001b[0;36mFTP.putline\u001b[0;34m(self, line)\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdebugging \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    197\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m*put*\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msanitize(line))\n\u001b[0;32m--> 198\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msendall\u001b[49m(line\u001b[38;5;241m.\u001b[39mencode(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoding))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'sendall'"
     ]
    }
   ],
   "source": [
    "zipped = zipfilem(ckpt2,zipped_filename)\n",
    "st = sendfile('ckpt_wt230407_201.zip')\n",
    "print(f'file {zipped} from {ckpt2} ; {st}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76ff5af-5ffb-4fc4-bb64-51eb63e1933f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9fcef6-d449-4fa7-b94f-571b6b9eb2d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
